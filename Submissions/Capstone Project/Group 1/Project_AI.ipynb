{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. INSTALL LIBRARIES\n",
        "!pip install -q -U langchain-core langchain-community langchain-groq pypdf chromadb tiktoken langchain-text-splitters langchain-classic sentence-transformers\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import uuid\n",
        "import base64\n",
        "import re\n",
        "from google.colab import files\n",
        "from IPython.display import display, Markdown, Image\n",
        "\n",
        "# LangChain Imports\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# 2. SETUP GROQ API KEY\n",
        "GROQ_API_KEY = \"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\"\n",
        "\n",
        "def render_mermaid(full_text):\n",
        "    \"\"\"Regex-based cleaner to extract and render Mermaid code visually.\"\"\"\n",
        "    # Find the block starting with graph TD and ending with the last bracket\n",
        "    match = re.search(r'(graph\\s+TD.*\\])', full_text, re.DOTALL)\n",
        "    if not match:\n",
        "        print(\"Assistant provided code, but it didn't match the 'graph TD' format for visualization.\")\n",
        "        return\n",
        "\n",
        "    code = match.group(1).strip()\n",
        "\n",
        "    # Use UTF-8 for special characters and encode for the mermaid.ink API\n",
        "    try:\n",
        "        code_bytes = code.encode(\"utf-8\")\n",
        "        base64_bytes = base64.b64encode(code_bytes)\n",
        "        base64_string = base64_bytes.decode(\"ascii\")\n",
        "        print(\"\\n--- Visual Flowchart ---\")\n",
        "        display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
        "    except Exception as e:\n",
        "        print(f\"Visualization Error: {e}\")\n",
        "\n",
        "def run_research_assistant():\n",
        "    print(\"--- AI Research Assistant Project (MatSoc) ---\")\n",
        "    db_id = str(uuid.uuid4())[:8]\n",
        "    persist_dir = f\"./chroma_db_{db_id}\"\n",
        "\n",
        "    # 3. UPLOAD PDF\n",
        "    print(\"\\nStep 1: Upload your Research Paper (PDF)\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded: return\n",
        "    file_path = list(uploaded.keys())[0]\n",
        "\n",
        "    # 4. PROCESS PDF\n",
        "    print(\"Step 2: Processing document...\")\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    docs = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    # 5. LOCAL EMBEDDINGS (Deviation from Baseline)\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=persist_dir)\n",
        "\n",
        "    # 6. LLM SETUP (Groq)\n",
        "    llm = ChatGroq(temperature=0, groq_api_key=GROQ_API_KEY, model_name=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "    template = \"\"\"You are an AI Research Assistant.\n",
        "    Answer ONLY using the provided context. If the answer is not in the context, say you don't know.\n",
        "\n",
        "    IF the user asks for a flowchart:\n",
        "    1. Briefly explain the steps in text.\n",
        "    2. Provide the Mermaid JS code starting with 'graph TD'.\n",
        "\n",
        "    Context: {context}\n",
        "    Question: {question}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "    # 7. RAG PIPELINE\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm,\n",
        "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
        "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        "    )\n",
        "\n",
        "    # 8. CHAT INTERFACE\n",
        "    print(\"\\n--- System Ready! ---\")\n",
        "    while True:\n",
        "        query = input(\"\\nYour Question: \")\n",
        "        if query.lower() == 'exit':\n",
        "            shutil.rmtree(persist_dir, ignore_errors=True)\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            response = qa_chain.invoke(query)\n",
        "            result = response['result']\n",
        "            display(Markdown(f\"**Assistant:** {result}\"))\n",
        "\n",
        "            if \"graph TD\" in result:\n",
        "                render_mermaid(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "# RUN\n",
        "run_research_assistant()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3el7L6VgUHZf",
        "outputId": "447e05b0-5668-4c9f-dd8f-35979539f9d8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- AI Research Assistant Project (MatSoc) ---\n",
            "\n",
            "Step 1: Upload your Research Paper (PDF)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-49fd333c-e053-4db9-8137-490179fc4d18\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-49fd333c-e053-4db9-8137-490179fc4d18\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving DuanZhang_Separation_TASLP.pdf to DuanZhang_Separation_TASLP.pdf\n",
            "Step 2: Processing document...\n",
            "\n",
            "--- System Ready! ---\n",
            "\n",
            "Your Question: Give me a flowchart of this paper.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:** The steps in the flowchart are as follows: \n1. Calculate the SDR curve of the mixed signal for both the piccolo source and the voice source.\n2. Compare the calculated SDR curves with the oracle lines, which represent the theoretical upper bounds of single-channel source separation performance.\n3. Use the AHS model to calculate the minus log-likelihood of the piccolo signal and the voice signal.\n4. Evaluate the performance of the algorithm by comparing the MPE results with the true MPE results and the results from [53].\n\nHere is the Mermaid JS code for the flowchart:\n```graph TD\n    A[Calculate SDR curve of mixed signal] --> B[Compare with oracle lines]\n    B --> C[Calculate minus log-likelihood using AHS model]\n    C --> D[Evaluate performance by comparing MPE results]\n    D --> E[Compare with true MPE results and [53]]\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Visual Flowchart ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://mermaid.ink/img/Z3JhcGggVEQKICAgIEFbQ2FsY3VsYXRlIFNEUiBjdXJ2ZSBvZiBtaXhlZCBzaWduYWxdIC0tPiBCW0NvbXBhcmUgd2l0aCBvcmFjbGUgbGluZXNdCiAgICBCIC0tPiBDW0NhbGN1bGF0ZSBtaW51cyBsb2ctbGlrZWxpaG9vZCB1c2luZyBBSFMgbW9kZWxdCiAgICBDIC0tPiBEW0V2YWx1YXRlIHBlcmZvcm1hbmNlIGJ5IGNvbXBhcmluZyBNUEUgcmVzdWx0c10KICAgIEQgLS0+IEVbQ29tcGFyZSB3aXRoIHRydWUgTVBFIHJlc3VsdHMgYW5kIFs1M11d\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Your Question: mermaid diagram.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:** To create a flowchart for the provided context, we first need to identify the steps involved in the process. The context discusses the separation of harmonic instrumental sources and singing voices using the AHS (Amplitude Harmonic Structure) model. The steps can be briefly explained as follows:\n\n1. Generate a mixed signal by adding the two sources (e.g., piccolo and voice, or oboe and euphonium) with equal energy or a specified energy ratio.\n2. Learn the AHS models from the mixed signal for each source.\n3. Use the learned AHS models to separate the sources from the mixed signal.\n4. Evaluate the performance of the separation using metrics such as SDR (Signal-to-Distortion Ratio).\n\nHere is the Mermaid JS code for the flowchart:\n```mermaid\ngraph TD\n    A[Generate Mixed Signal] --> B[Learn AHS Models]\n    B --> C[Separate Sources]\n    C --> D[Evaluate Performance]\n    D --> E[Output Results]\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Visual Flowchart ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://mermaid.ink/img/Z3JhcGggVEQKICAgIEFbR2VuZXJhdGUgTWl4ZWQgU2lnbmFsXSAtLT4gQltMZWFybiBBSFMgTW9kZWxzXQogICAgQiAtLT4gQ1tTZXBhcmF0ZSBTb3VyY2VzXQogICAgQyAtLT4gRFtFdmFsdWF0ZSBQZXJmb3JtYW5jZV0KICAgIEQgLS0+IEVbT3V0cHV0IFJlc3VsdHNd\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Your Question: bnanan\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:** I don't know."
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Your Question: g\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Assistant:** I don't know. There is no question provided."
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Your Question: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAbB-kxPWKGn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}